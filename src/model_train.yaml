log_file : "../log/model_train.log"

train_files:  # 只读第一个
  # 全量股票
  # - ../training_data/data.bin.20221123_2106
  # 1000个股票, 365天数据
  # - ../training_data/data.bin.20221128_1646
  # 测试
  # - ../training_data/data.bin.20221129_1633  # fix pe/turnover
  - ../training_data/data.bin.20221130_1802
#预处理instance: 处理label, 过滤fid等
train_data:
  validate_date : "20220930"      # 验证集: 没有label且时间在这个后面
  label:
    method : binarize
    args: 
      # key : next_14d_mean_price   #
      key : next_14d_close_price   #
      threshold : 0.0   #
  min_fid_occurrence : 0    # 每个fid最少出现几次，低于这个次数，程序直接退出报错
  debug:                 # debug 函数
    fid_whitelist:                # 要debug的fid, 训练的时候也只会训练这些
      # - 1444932513133819623
      # - 1444015990897181305
      # - 1450672333743350316
      # - 1453955548291422233
    slot_whitelist: 
      # - 5
      # - 101
      # - 102
      # - 129
      # - 130
      # - 132
      # - 133
      # - 135
    slot_blacklist:
      # - 1
      # - 2
      # - 3
      # - 4
    


model: 
  loss_type: cross_entropy
  # global_bias : True
  optimizer : 
    # type: AdagradOptimizer
    # type: MomentumOptimizer
    # momentum : 0.9       # 动量

    type: GradientDescentOptimizer  # 效果最好
  learning_rate: 0.001
  mini_batch:
    batch_size: 1000
    epoch : 50000
  bias_nn_dims: [16,1]
# 调参经验： 
#  1. learning rate (随着特征增加，原来适合的lr可能变得不适合)
#      1e-2/1e-3: 太高了，直接学飞
#      1e-4: 0.037
#      1e-4: 0.085 会导致稀疏特征学得慢
#  2. l2_lambda (看着没什么用)
#      0 :    diff = 0.025
#      1e-3 : diff = 0.035
#      1e-4 : diff = 0.032
#      1e-5 : diff = 0.042
#  3. 有无global_bias diff 0.03 vs 0.02
#  4. adagrad: 学习率要调整为0.01, 但是扔没有明显收益
