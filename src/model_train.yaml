log_file : "../log/model_train.log"

# max_ins: 100000   # debug，最多读多少数据
train_files:  # 只读第一个
  # 全量股票
  # - ../training_data/data.weekly.20230131_1918
  - ../training_data/data.daily.20230206_1746
  - ../training_data/data.daily.20230203_2003
  - ../training_data/data.daily.20230202_2207
#预处理instance: 处理label, 过滤fid等
train_data:
  train_ins_percent: 0.8          # 训练集占比(优先于下面这个参数)
  # validate_date : "20220101"      # 验证集: 没有label且时间在这个后面
  label:
    args: 
      key : next_14d_close_price
  min_fid_occurrence : 0    # 每个fid最少出现几次，低于这个次数，程序直接退出报错
  debug:                 # debug 函数
    fid_whitelist:                # 要debug的fid, 训练的时候也只会训练这些
      # - 1444932513133819623
      # - 1444015990897181305
      # - 1450672333743350316
      # - 1453955548291422233
    slot_whitelist: 
      # - 5
    slot_blacklist:
      - 82
      - 83
      - 11
      - 240
      - 241
      - 242
      - 243
      - 244


model: 
  label:
    binarized: 0.03
    # binarized: date_threshold
  loss_type: cross_entropy
  # loss_type: mse
  # global_bias : True
  optimizer : 
    # type: AdagradOptimizer
    # type: MomentumOptimizer
    # momentum : 0.9       # 动量

    type: GradientDescentOptimizer  # 效果最好
  learning_rate: 0.001
  mini_batch:
    batch_size: 1000
    epoch : 50000
  bias_nn_dims: [16,1]
  bias_attention: True
  # dense_30d: True
# 调参经验： 
#  1. learning rate (随着特征增加，原来适合的lr可能变得不适合)
#      1e-2/1e-3: 太高了，直接学飞
#      1e-4: 0.037
#      1e-4: 0.085 会导致稀疏特征学得慢
#  2. l2_lambda (看着没什么用)
#      0 :    diff = 0.025
#      1e-3 : diff = 0.035
#      1e-4 : diff = 0.032
#      1e-5 : diff = 0.042
#  3. 有无global_bias diff 0.03 vs 0.02
#  4. adagrad: 学习率要调整为0.01, 但是扔没有明显收益
