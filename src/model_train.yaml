log_file : "../log/model_train.log"

# max_ins: 10000   # debug，最多读多少数据
train_files:  # 只读第一个
  # 全量股票
  - ../training_data/data.bin.20230121_2038
#预处理instance: 处理label, 过滤fid等
train_data:
  validate_date : "20221001"      # 验证集: 没有label且时间在这个后面
  label:
    method : binarize
    args: 
      # key : next_14d_mean_price   #
      key : next_14d_close_price   #
      # key : next_7d_14d_mean_price
      # threshold : 0.03   #
      threshold_by_day: True
  min_fid_occurrence : 0    # 每个fid最少出现几次，低于这个次数，程序直接退出报错
  debug:                 # debug 函数
    fid_whitelist:                # 要debug的fid, 训练的时候也只会训练这些
      # - 1444932513133819623
      # - 1444015990897181305
      # - 1450672333743350316
      # - 1453955548291422233
    slot_whitelist: 
      # - 5
      # - 101
      # - 102
      # - 129
      # - 130
      # - 132
      # - 133
      # - 135
    slot_blacklist:
      - 82
      - 83
      - 11
      - 240
      - 241
      - 242
      - 243
      - 244
      # - 1
      # - 5 
      # - 49
      # - 52
      # - 53
      # - 60
      # - 62
      # - 63
      # - 64
      # - 80
      # - 81
      # - 90
      # - 92
      # - 93 
      # - 101
      # - 104
      # - 126
      # - 128
      # - 129
      # - 134 
      # - 136
      # - 139
      # - 150
      # - 151
      # - 201 
      # - 222


model: 
  loss_type: cross_entropy
  # loss_type: mse
  # global_bias : True
  optimizer : 
    # type: AdagradOptimizer
    # type: MomentumOptimizer
    # momentum : 0.9       # 动量

    type: GradientDescentOptimizer  # 效果最好
  learning_rate: 0.001
  mini_batch:
    batch_size: 1000
    epoch : 50000
  bias_nn_dims: [16,1]
  bias_attention: True
  # dense_30d: True
# 调参经验： 
#  1. learning rate (随着特征增加，原来适合的lr可能变得不适合)
#      1e-2/1e-3: 太高了，直接学飞
#      1e-4: 0.037
#      1e-4: 0.085 会导致稀疏特征学得慢
#  2. l2_lambda (看着没什么用)
#      0 :    diff = 0.025
#      1e-3 : diff = 0.035
#      1e-4 : diff = 0.032
#      1e-5 : diff = 0.042
#  3. 有无global_bias diff 0.03 vs 0.02
#  4. adagrad: 学习率要调整为0.01, 但是扔没有明显收益
